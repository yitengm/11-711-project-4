{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2m3aBfMUx0T"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "I would like to explore emotion prediction in Chinese domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFGWXTLiZFNw"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install tqdm scikit-learn datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQLzWSanKj5z"
      },
      "outputs": [],
      "source": [
        "#@title Import Package\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import numpy as np\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "import multiprocessing as mp\n",
        "from importlib import reload\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import (WEIGHTS_NAME,\n",
        "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
        "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "from transformers import BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModel\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dFf8GqtAQNhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdygpIH_U-sM"
      },
      "outputs": [],
      "source": [
        "#@title Access data\n",
        "test_split = pd.read_csv(\"/content/drive/MyDrive/NLP/Project4/Model/cn-data-revised/test_split.csv\")\n",
        "\n",
        "train_split = pd.read_csv(\"/content/drive/MyDrive/NLP/Project4/Model/cn-data-revised/train_split.csv\")\n",
        "valid_split = pd.read_csv(\"/content/drive/MyDrive/NLP/Project4/Model/cn-data-revised/valid_split.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xI-W-oOVCwp"
      },
      "outputs": [],
      "source": [
        "train_split.head(5) # 'Emotion', 'Utterance'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def joinTextBySpeakerAndEmotion(df):\n",
        "\n",
        "  # Initialize the group column\n",
        "  df['group'] = 0\n",
        "  current_group = 1\n",
        "\n",
        "  # Iterate through the DataFrame\n",
        "  for i in range(len(df) - 1):\n",
        "      df.at[i, 'group'] = current_group\n",
        "      # print(current_group)\n",
        "\n",
        "      # Check if the next row should be in the same group\n",
        "      if df.at[i, 'Speaker'] != df.at[i + 1, 'Speaker'] or df.at[i, 'Emotion'] != df.at[i + 1, 'Emotion']:\n",
        "          current_group += 1\n",
        "\n",
        "  # Assign group number to the last row\n",
        "  df.at[len(df) - 1, 'group'] = current_group\n",
        "\n",
        "  # Now group by 'group', 'Speaker', and 'Emotion', and combine 'translation'\n",
        "  # grouped = df.groupby(['group', 'Speaker', 'Emotion']).agg({\n",
        "  grouped = df.groupby(['group']).agg({\n",
        "      'Utterance' : lambda y: ', '.join(y) + '.',\n",
        "      # For other columns, you can decide how to aggregate them\n",
        "      'TV_ID': 'first',\n",
        "      'Dialogue_ID': 'first',\n",
        "      'Utterance_ID': 'first',\n",
        "      'Speaker': 'first',\n",
        "      'Emotion': 'first'\n",
        "      # ... handle other columns as needed\n",
        "  }).reset_index(drop=True)\n",
        "\n",
        "  # The resulting DataFrame\n",
        "  return grouped"
      ],
      "metadata": {
        "id": "vvxZyziCPK47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = joinTextBySpeakerAndEmotion(train_split)\n",
        "valid_split = joinTextBySpeakerAndEmotion(valid_split)\n",
        "test_split  = joinTextBySpeakerAndEmotion(test_split)"
      ],
      "metadata": {
        "id": "u0gAIWzMPRwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('number of train data:', len(train_split))\n",
        "print('number of validation data:', len(valid_split))\n",
        "print('number of test data:', len(test_split))"
      ],
      "metadata": {
        "id": "DR4MeCsuXXph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert pandas dataframes to Hugging Face Dataset\n",
        "# Select only the 'Utterance' and 'Emotion' columns\n",
        "train_split = train_split[['Utterance', 'Emotion']]\n",
        "valid_split = valid_split[['Utterance', 'Emotion']]\n",
        "test_split  = test_split[['Utterance', 'Emotion']]\n",
        "\n",
        "# randomly select 1/3 data from training set\n",
        "random_seed = 42\n",
        "# Randomly sample train:valid = 4:1\n",
        "train_split = train_split.sample(frac=3/4, random_state=random_seed)\n",
        "# valid_split = valid_split.sample(frac=3/4, random_state=random_seed)\n",
        "\n",
        "print('number of train data:', len(train_split))\n",
        "print('number of validation data:', len(valid_split))\n",
        "print('number of test data:', len(test_split))\n",
        "\n",
        "# map emotions astonished ->\n",
        "'''\n",
        "id2label = {\n",
        "    0: 'admiration',\n",
        "    1: 'amusement',\n",
        "    2: 'anger',\n",
        "    3: 'annoyance',\n",
        "    4: 'approval',\n",
        "    5: 'astonished',  # not in GoEmotion\n",
        "    6: 'caring',\n",
        "    7: 'confusion',\n",
        "    8: 'curiosity',\n",
        "    9: 'depress',     # not in GoEmotion\n",
        "    10: 'desire',\n",
        "    11: 'disappointment',\n",
        "    12: 'disapproval',\n",
        "    13: 'disgust',\n",
        "    14: 'embarrassment',\n",
        "    15: 'excitement',\n",
        "    16: 'fear',\n",
        "    17: 'grateful',   # not in GoEmotion\n",
        "    18: 'gratitude',\n",
        "    19: 'grief',\n",
        "    20: 'happy',      # not in GoEmotion\n",
        "    21: 'joy',\n",
        "    22: 'love',\n",
        "    23: 'nervousness',\n",
        "    24: 'neutral',\n",
        "    25: 'optimism',\n",
        "    26: 'pride',\n",
        "    27: 'realization',\n",
        "    28: 'relaxed',    # not in GoEmotion\n",
        "    29: 'relief',\n",
        "    30: 'remorse',\n",
        "    31: 'sadness',\n",
        "    32: 'surprise',\n",
        "    33: 'worried'   # not in GoEmotion\n",
        "  }\n",
        "'''\n",
        "# Define the mapping of old values to new values\n",
        "emotion_replacements = {\n",
        "    'happy': 'joy',\n",
        "    'grateful': 'gratitude',\n",
        "    'relaxed': 'relief',\n",
        "    'depress': 'grief',\n",
        "    'astonished': 'surprise',\n",
        "    'worried': 'nervousness'\n",
        "}\n",
        "\n",
        "# Apply the replacements to the 'Emotion' column of each DataFrame\n",
        "train_split['Emotion'] = train_split['Emotion'].replace(emotion_replacements)\n",
        "valid_split['Emotion'] = valid_split['Emotion'].replace(emotion_replacements)\n",
        "test_split['Emotion'] = test_split['Emotion'].replace(emotion_replacements)\n",
        "\n",
        "# rename columns\n",
        "train_split = train_split.rename(columns={\"Utterance\": \"text\", \"Emotion\": \"labels\"})\n",
        "valid_split = valid_split.rename(columns={\"Utterance\": \"text\", \"Emotion\": \"labels\"})\n",
        "test_split = test_split.rename(columns={\"Utterance\": \"text\", \"Emotion\": \"labels\"})\n",
        "\n",
        "# convert to numberical labels\n",
        "# Create label2id and id2label dictionaries\n",
        "# label2id = {label: [idx] for idx, label in enumerate(train_split['labels'].unique())}\n",
        "# id2label = {map(int,idx): label for label, idx in label2id.items()}\n",
        "\n",
        "id2label = {0:\"admiration\",\n",
        "            1:\"amusement\",\n",
        "            2:\"anger\",\n",
        "            3:\"annoyance\",\n",
        "            4:\"approval\",\n",
        "            5:\"caring\",\n",
        "            6:\"confusion\",\n",
        "            7:\"curiosity\",\n",
        "            8:\"desire\",\n",
        "            9:\"disappointment\",\n",
        "            10:\"disapproval\",\n",
        "            11:\"disgust\",\n",
        "            12:\"embarrassment\",\n",
        "            13:\"excitement\",\n",
        "            14:\"fear\",\n",
        "            15:\"gratitude\",\n",
        "            16:\"grief\",\n",
        "            17:\"joy\",\n",
        "            18:\"love\",\n",
        "            19:\"nervousness\",\n",
        "            20:\"optimism\",\n",
        "            21:\"pride\",\n",
        "            22:\"realization\",\n",
        "            23:\"relief\",\n",
        "            24:\"remorse\",\n",
        "            25:\"sadness\",\n",
        "            26:\"surprise\",\n",
        "            27:\"neutral\"}\n",
        "label2id = {label: [idx] for idx, label in id2label.items()}\n",
        "# print(label2id)\n",
        "\n",
        "# convert labels in the dataframes to numerical labels\n",
        "train_split['labels'] = train_split['labels'].map(label2id)\n",
        "valid_split['labels'] = valid_split['labels'].map(label2id)\n",
        "test_split['labels'] = test_split['labels'].map(label2id)\n",
        "\n",
        "from datasets import Dataset\n",
        "# create data set\n",
        "train_dataset = Dataset.from_pandas(train_split)\n",
        "valid_dataset = Dataset.from_pandas(valid_split)\n",
        "test_dataset = Dataset.from_pandas(test_split)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "dataset = {\"train\": train_dataset, \"validation\": valid_dataset, \"test\" : test_dataset}"
      ],
      "metadata": {
        "id": "UU6r6wJv3Tgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# goEmotion data\n",
        "# goEmotion = load_dataset(\"go_emotions\", \"simplified\")\n",
        "# print(Counter([x['labels'].__len__() for x in goEmotion['train']]))\n",
        "# print(goEmotion.keys())\n",
        "# print(goEmotion['train'][0])\n",
        "# print(\"------\")\n",
        "# current Chinese data, compared with goEmotion data\n",
        "# @title look into dataset\n",
        "# print(Counter([x['labels'].__len__() for x in dataset['train']]))\n",
        "print(dataset.keys())\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "1zAUXfb84ska"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXfvK7xVVFxe"
      },
      "source": [
        "#### Baseline Model\n",
        "\n",
        "I would like to reuse Roberta Model from project 3. First, I will convert Chinese text into English text. Then, I will be able to use the Roberta model directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kfIM_L0_2QT"
      },
      "source": [
        "#### Advanced Model\n",
        "\n",
        "Use model that is trained directly on Chinese text. https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEBZKFJUZgPD"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load Chinese tokenizer\n",
        "MODEL_NAME = \"hfl/chinese-roberta-wwm-ext-large\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "num_labels = len(label2id)\n",
        "# Load the base BERT model\n",
        "model = BertModel.from_pretrained(MODEL_NAME)\n",
        "#model = BertForSequenceClassification.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I3ZkFGXVNru"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class emotionDataset(Dataset):\n",
        "    \"\"\"Class to load the dataset and get batches of paras\"\"\"\n",
        "\n",
        "    def __init__(self, list_data,\n",
        "                 tokenizer, max_length):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data = list_data\n",
        "        self.pad_token = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return length of dataset.\"\"\"\n",
        "        return self.data.__len__()\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"Return sample from dataset at index i.\"\"\"\n",
        "        example = self.data[i]\n",
        "        inputs = self.tokenizer.encode_plus(example['text'],\n",
        "                                            add_special_tokens=True,\n",
        "                                            truncation=True,\n",
        "                                            max_length=self.max_length)\n",
        "\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        input_ids = input_ids[:self.max_length]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        padding_length = self.max_length - len(input_ids)\n",
        "        input_ids = input_ids + ([self.pad_token] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == self.max_length, \"Error with input length {} vs {}\".format(len(input_ids), self.max_length)\n",
        "\n",
        "        nli_label = example['labels'][0]\n",
        "\n",
        "        return_dict = {'input_ids':torch.LongTensor(input_ids),\n",
        "                       'attention_mask':torch.LongTensor(attention_mask),\n",
        "                       'labels': torch.LongTensor([nli_label])}\n",
        "\n",
        "        return return_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_OH_ek3H3Wi"
      },
      "outputs": [],
      "source": [
        "# Train dataset\n",
        "train_dataset = emotionDataset(list_data=dataset['train'],\n",
        "                               tokenizer=tokenizer,\n",
        "                               max_length=100)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=mp.cpu_count())\n",
        "\n",
        "# Validation dataset\n",
        "val_dataset = emotionDataset(list_data=dataset['validation'],\n",
        "                             tokenizer=tokenizer,\n",
        "                             max_length=100)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                        num_workers=mp.cpu_count())\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = emotionDataset(list_data=dataset['test'],\n",
        "                               tokenizer=tokenizer,\n",
        "                               max_length=100)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                         shuffle=False, num_workers=mp.cpu_count())\n",
        "\n",
        "(train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = next(iter(train_loader))\n",
        "example_batch['input_ids'].shape, example_batch['attention_mask'].shape, example_batch['labels'].shape"
      ],
      "metadata": {
        "id": "jgKGp_4snJ2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tried different learning rate 1e-5, 2e-5\n",
        "'''\n",
        "args = {'weight_decay':0.0,\n",
        "        'learning_rate':2e-5,\n",
        "        'epochs':5,\n",
        "        'gradient_accumulation_steps':1,\n",
        "        'adam_epsilon':2e-8}\n",
        "args['t_total'] = len(train_loader) // args['gradient_accumulation_steps'] * args['epochs']\n",
        "args['warmup_steps'] = int(0.10*args['t_total'])\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args['weight_decay']},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'],\n",
        "                                            num_training_steps=args['t_total'])\n",
        "'''\n",
        "args = {\n",
        "    'learning_rate': 2e-5,  # Learning rate\n",
        "    'epochs': 5,            # Number of epochs\n",
        "    'weight_decay': 0.01,   # Weight decay for regularization\n",
        "    'adam_epsilon': 1e-8,   # Epsilon value for Adam optimizer\n",
        "    'warmup_steps': 1000,      # Number of warmup steps\n",
        "    'max_grad_norm': 1.0,   # Max gradient norm for gradient clipping\n",
        "    'dropout_rate': 0.1     # Dropout rate\n",
        "}\n",
        "\n",
        "# Adjust learning rate with a scheduler\n",
        "total_steps = len(train_loader) * args['epochs']\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args['weight_decay']},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=args['warmup_steps'],\n",
        "                                            num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "fkhTm1ie_czn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check if a GPU is available and if not, use a CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "1rBaNOODAaHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = [0.2, 0.2, 12, 13, 0.2, 0.2, 0.8, 1, 0.2, 0.8, 1.2, 6.2, 0.8, 0.2, 1.7, 0.4, 9.4, 3.3, 0.2, 6.2, 0.2, 0.2, 0.2, 6.8, 0.8, 1.9, 3.1, 29]\n",
        "total_count = sum(class_counts)\n",
        "class_weights = [total_count / class_count for class_count in class_counts]\n",
        "\n",
        "# Normalize the weights so that the smallest one is 1.0\n",
        "max_weight = max(class_weights)\n",
        "normalized_weights = [w / max_weight for w in class_weights]\n",
        "\n",
        "class_weights_tensor = torch.tensor(normalized_weights)\n",
        "\n",
        "class CustomBertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_labels,class_weights=class_weights_tensor):\n",
        "        super(CustomBertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.dense = nn.Linear(self.bert.config.hidden_size, 768)\n",
        "        self.elu = nn.ELU()\n",
        "        self.out_proj = nn.Linear(768, num_labels)\n",
        "\n",
        "        # If class weights are provided, use them to initialize CrossEntropyLoss\n",
        "        if class_weights is not None:\n",
        "            self.loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        else:\n",
        "            self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]\n",
        "        x = self.dropout(pooled_output)\n",
        "        x = self.dense(x)\n",
        "        x = self.elu(x)\n",
        "        logits = self.out_proj(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "    def set_dropout_rate(self, dropout_rate):\n",
        "        self.dropout.p = dropout_rate\n",
        "        self.bert.config.hidden_dropout_prob = dropout_rate\n",
        "        self.bert.config.attention_probs_dropout_prob = dropout_rate\n",
        "\n",
        "\n",
        "# Initialize the custom model:\n",
        "# Initialize the custom model with the base BERT model\n",
        "custom_model = CustomBertClassifier(model, num_labels)\n",
        "\n",
        "# Set the dropout rate\n",
        "dropout_rate = args['dropout_rate']\n",
        "custom_model.set_dropout_rate(dropout_rate)\n",
        "\n",
        "# Move the model to GPU and apply DataParallel\n",
        "model = nn.DataParallel(custom_model).to(device)"
      ],
      "metadata": {
        "id": "N0pMu6uj-6Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_save_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints/'  # model with config from proj3 that has little training improvement\n",
        "# model_save_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints2/'  # model fine tuned with two layers (128 ALU) added with lr = 1e-5\n",
        "# model_save_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints3' # model fine tuned with two layers (521 ELU) added with lr = 1e-5\n",
        "# model_save_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints4' # model fine tuned with two layers (768 ELU) added with lr = 1e-5 now drop out rate 01->0.6, and new loss func\n",
        "# model_save_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints5' # model fine tuned with two layers (768 ELU) added with lr = 2e-5 now drop out rate 0.2, and new loss func and new data\n",
        "model_save_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints6'\n",
        "os.makedirs(model_save_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "1wPxQ9hR1ir9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Checkpoints2  \n",
        "\n",
        "Epoch 1/5 - Train Loss: 2.3372, Val Loss: 2.4042\n",
        "\n",
        "Epoch 2/5 - Train Loss: 2.1814, Val Loss: 2.3931\n",
        "\n",
        "Epoch 3/5 - Train Loss: 2.0031, Val Loss: 2.4502\n",
        "\n",
        "Epoch 4/5 - Train Loss: 1.7970, Val Loss: 2.5422\n",
        "\n",
        "Epoch 5/5 - Train Loss: 1.6418, Val Loss: 2.6374\n",
        "\n",
        "2. Checkpoints3  \n",
        "\n",
        "Epoch 1/5 - Train Loss: 2.3058, Val Loss: 2.3740\n",
        "\n",
        "Epoch 2/5 - Train Loss: 2.1385, Val Loss: 2.3510\n",
        "\n",
        "Epoch 3/5 - Train Loss: 1.9602, Val Loss: 2.4255\n",
        "\n",
        "Epoch 4/5 - Train Loss: 1.7630, Val Loss: 2.5286\n",
        "\n",
        "Next change the loss function to take into account the imbalanced data in the dataset.  \n",
        "\n",
        "3. Checkpoints4  \n",
        "\n",
        "Epoch 1/8 - Train Loss: 3.2579, Val Loss: 3.2368\n",
        "\n",
        "Epoch 2/8 - Train Loss: 3.0837, Val Loss: 3.2269\n",
        "\n",
        "Epoch 3/8 - Train Loss: 2.7848, Val Loss: 3.3330\n",
        "\n",
        "Epoch 4/8 - Train Loss: 2.4037, Val Loss: 3.4085\n",
        "\n",
        "Epoch 5/8 - Train Loss: 2.0744, Val Loss: 3.4990\n",
        "\n",
        "Epoch 6/8 - Train Loss: 1.8241, Val Loss: 3.5701\n",
        "\n",
        "Epoch 7/8 - Train Loss: 1.6487, Val Loss: 3.6001\n",
        "\n",
        "Epoch 8/8 - Train Loss: 1.5383, Val Loss: 3.6185"
      ],
      "metadata": {
        "id": "Zual0WvYjFaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Assuming args, model, train_loader, val_loader, optimizer, scheduler, device, and model_save_path are defined\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for each_epoch in range(args['epochs']):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        model.zero_grad()\n",
        "        loss, logits = model(**batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            loss, logits = model(**batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {each_epoch + 1}/{args['epochs']} - \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    epoch_save_path = os.path.join(model_save_path, f'model_epoch_{each_epoch+1}.pt')\n",
        "    torch.save(model.state_dict(), epoch_save_path)\n",
        "    print(f\"Saved model checkpoint to {epoch_save_path}\")"
      ],
      "metadata": {
        "id": "UUZnNe_qcYId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training and validation losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, args['epochs'] + 1), train_losses, label='Training Loss', marker='o')\n",
        "plt.plot(range(1, args['epochs'] + 1), val_losses, label='Validation Loss', marker='o')\n",
        "\n",
        "plt.title('Training and Validation Loss Per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PQsaojPocY0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# Load model\n",
        "base_bert_model = BertModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
        "custom_model = CustomBertClassifier(base_bert_model, num_labels)\n",
        "# Wrap the model with nn.DataParallel and move to the device\n",
        "model = nn.DataParallel(custom_model).to(device)\n",
        "\n",
        "# Load the saved model weights\n",
        "model_path = '/content/drive/MyDrive/NLP/Project4/Checkpoints5/model_epoch_2.pt'\n",
        "# Load the saved model weights\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move the model to GPU and apply DataParallel if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = nn.DataParallel(custom_model).to(device)"
      ],
      "metadata": {
        "id": "34BfDnckjy-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, num_samples=10):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a test dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to be evaluated.\n",
        "        test_loader: DataLoader for the test dataset.\n",
        "        device: The device (CPU/GPU) to run the evaluation on.\n",
        "        num_samples: Number of samples to print for inspection.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the classification report.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    sample_outputs = []\n",
        "\n",
        "    with torch.no_grad():  # No need to track the gradients\n",
        "        for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\", leave=False)):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            logits = outputs[1]\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            labels = labels.to('cpu').numpy()\n",
        "\n",
        "            # Convert logits to predictions\n",
        "            preds = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.flatten())\n",
        "\n",
        "            # Save some sample outputs for inspection\n",
        "            if i < num_samples:\n",
        "                sample_outputs.append((preds, labels.flatten()))\n",
        "\n",
        "    # Print sample predictions\n",
        "    for j, (pred, label) in enumerate(sample_outputs):\n",
        "        print(f\"Sample {j+1}:\")\n",
        "        print(f\"Predictions: {pred}\")\n",
        "        print(f\"True Labels: {label}\\n\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    return classification_report(all_preds, all_labels, target_names=list(id2label.values()), digits=4)\n",
        "\n",
        "# Evaluate the model\n",
        "report = evaluate_model(model, test_loader, device)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "SV3cql6tk72d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}